{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../assignment-1/data/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding groups\n",
    "\n",
    "#property_type_groups = [['Apartment','Serviced apartment'], ['House','Townhouse','Chalet'],['Condominium'],['Guesthouse','Guest suite'],['Loft'],['Bed & Breakfast'],['Other','Boutique hotel','Hostel','Camper/RV','Castle','Boat','Timeshare'],['Villa'],['Cabine','Earth House','Yurt','Dorm','Tent']]\n",
    "property_type_groups = [[property_type] for property_type in data['property_type'].dropna().str.split(', ').explode().unique().tolist()]\n",
    "#property_room_type_groups = [['Private room'],['Entire home/apt'],['Shared room']]\n",
    "property_room_type_groups = [[property_room_type] for property_room_type in data['property_room_type'].dropna().str.split(', ').explode().unique().tolist()]\n",
    "#booking_cancel_policy_groups = [['flexible'],['moderate'],['strict', 'super_strict_30']]\n",
    "booking_cancel_policy_groups = [[booking_cancel_policy] for booking_cancel_policy in data['booking_cancel_policy'].dropna().str.split(', ').explode().unique().tolist()]\n",
    "#property_bed_type_groups = [['Couch','Airbed','Futon','Pull-out Sofa'],['Real Bed']]\n",
    "property_bed_type_groups = [[property_bed_type] for property_bed_type in data['property_bed_type'].dropna().str.split(', ').explode().unique().tolist()]\n",
    "host_response_time_groups = [['a few days or more'],['within a day'],['within a few hours'],['within an hour'],['']]\n",
    "amenity_groups = [[amenity] for amenity in data['property_amenities'].dropna().str.split(', ').explode().unique().tolist()]  # Each ammenity is a separate group\n",
    "host_verified_groups = [[verification] for verification in data['host_verified'].dropna().str.split(', ').explode().unique().tolist()]  # Each host_verified status is a separate group\n",
    "extra_groups = [[extra] for extra in data['extra'].dropna().str.split(', ').explode().unique().tolist()]  # Each extra comment is a separate group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(data):\n",
    "\n",
    "    df = data.copy()\n",
    "\n",
    "    # Replace NaN values with empty string\n",
    "    df[['host_response_time','property_amenities','host_verified','extra']] = df[['host_response_time','property_amenities','host_verified','extra']].fillna('')\n",
    "\n",
    "    # Count amenities\n",
    "    df['amenities_count'] = df['property_amenities'].apply(lambda x: 0 if pd.isna(x) else x.count(',') + 1)\n",
    "\n",
    "    # Property city \n",
    "    df['property_city'] = df['property_lat'].apply(lambda x: 1 if x > 51 else 0)\n",
    "\n",
    "    # OneHotEncoding\n",
    "    for group in property_type_groups:\n",
    "        group_name = 'property_type' + '_' + group[0]\n",
    "        df[group_name] = df['property_type'].isin(group).astype(int)\n",
    "    for group in property_room_type_groups:\n",
    "        group_name = 'property_room_type' + '_' + group[0]\n",
    "        df[group_name] = df['property_room_type'].isin(group).astype(int)\n",
    "    for group in booking_cancel_policy_groups:\n",
    "        group_name = 'booking_cancel_policy' + '_' + group[0]\n",
    "        df[group_name] = df['booking_cancel_policy'].isin(group).astype(int)\n",
    "    for group in property_bed_type_groups:\n",
    "        group_name = 'property_bed_type' + '_' + group[0]\n",
    "        df[group_name] = df['property_bed_type'].isin(group).astype(int)\n",
    "    for group in host_response_time_groups:\n",
    "        group_name = 'host_response_time' + '_' + group[0]\n",
    "        df[group_name] = df['host_response_time'].isin(group).astype(int)\n",
    "\n",
    "    # MultiLabelEncoding\n",
    "    for group in amenity_groups:\n",
    "        new_col = df['property_amenities'].apply(lambda x: 1 if len(set(x.split(', ')).intersection(group)) > 0 else 0)\n",
    "        group_name = 'property_amenities' + '_' + group[0]\n",
    "        df = pd.concat([df, new_col.rename(group_name)], axis=1)\n",
    "    for group in host_verified_groups:\n",
    "        new_col = df['host_verified'].apply(lambda x: 1 if len(set(x.split(', ')).intersection(group)) > 0 else 0)\n",
    "        group_name = 'host_verified' + '_' + group[0]\n",
    "        df = pd.concat([df, new_col.rename(group_name)], axis=1)\n",
    "    for group in extra_groups:\n",
    "        new_col = df['extra'].apply(lambda x: 1 if len(set(x.split(', ')).intersection(group)) > 0 else 0)\n",
    "        group_name = 'extra' + '_' + group[0]\n",
    "        df = pd.concat([df, new_col.rename(group_name)], axis=1)\n",
    "\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_preprocessed = preprocessing(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ... (step 1 of 3) Processing standard_scaler, total=   0.1s\n",
      "[Pipeline] ...... (step 2 of 3) Processing mean_imputer, total=   0.0s\n",
      "[Pipeline] .......... (step 3 of 3) Processing en_model, total=   0.0s\n",
      "[Pipeline] ... (step 1 of 3) Processing standard_scaler, total=   0.1s\n",
      "[Pipeline] ...... (step 2 of 3) Processing mean_imputer, total=   0.0s\n",
      "[Pipeline] .......... (step 3 of 3) Processing en_model, total=   0.0s\n",
      "[Pipeline] ... (step 1 of 3) Processing standard_scaler, total=   0.1s\n",
      "[Pipeline] ...... (step 2 of 3) Processing mean_imputer, total=   0.0s\n",
      "[Pipeline] .......... (step 3 of 3) Processing en_model, total=   0.0s\n",
      "[Pipeline] ... (step 1 of 3) Processing standard_scaler, total=   0.1s\n",
      "[Pipeline] ...... (step 2 of 3) Processing mean_imputer, total=   0.0s\n",
      "[Pipeline] .......... (step 3 of 3) Processing en_model, total=   0.0s\n",
      "[Pipeline] ... (step 1 of 3) Processing standard_scaler, total=   0.0s\n",
      "[Pipeline] ...... (step 2 of 3) Processing mean_imputer, total=   0.0s\n",
      "[Pipeline] .......... (step 3 of 3) Processing en_model, total=   0.0s\n",
      "[Pipeline] ... (step 1 of 3) Processing standard_scaler, total=   0.0s\n",
      "[Pipeline] ...... (step 2 of 3) Processing mean_imputer, total=   0.0s\n",
      "[Pipeline] .......... (step 3 of 3) Processing en_model, total=   0.0s\n",
      "[Pipeline] ... (step 1 of 3) Processing standard_scaler, total=   0.1s\n",
      "[Pipeline] ...... (step 2 of 3) Processing mean_imputer, total=   0.0s\n",
      "[Pipeline] .......... (step 3 of 3) Processing en_model, total=   0.0s\n",
      "[Pipeline] ... (step 1 of 3) Processing standard_scaler, total=   0.1s\n",
      "[Pipeline] ...... (step 2 of 3) Processing mean_imputer, total=   0.0s\n",
      "[Pipeline] .......... (step 3 of 3) Processing en_model, total=   0.0s\n",
      "[Pipeline] ... (step 1 of 3) Processing standard_scaler, total=   0.1s\n",
      "[Pipeline] ...... (step 2 of 3) Processing mean_imputer, total=   0.0s\n",
      "[Pipeline] .......... (step 3 of 3) Processing en_model, total=   0.0s\n",
      "[Pipeline] ... (step 1 of 3) Processing standard_scaler, total=   0.1s\n",
      "[Pipeline] ...... (step 2 of 3) Processing mean_imputer, total=   0.0s\n",
      "[Pipeline] .......... (step 3 of 3) Processing en_model, total=   0.0s\n",
      "Cross-validation scores: [46.6166449  48.40761137 53.47920038 62.08065177 56.65453236 61.02939017\n",
      " 43.43216322 48.55917551 45.45718601 49.71646674]\n",
      "RMSE: 51.543302243042774\n"
     ]
    }
   ],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Keep only numerical columns\n",
    "data_preprocessed = data_preprocessed.select_dtypes(include=['int64', 'float64'])\n",
    "\n",
    "mean_imputer = SimpleImputer(strategy='mean')\n",
    "standard_scaler = StandardScaler()\n",
    "en_model = ElasticNet(alpha=1000, l1_ratio=0.7)\n",
    "\n",
    "# Define the full pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('standard_scaler', standard_scaler),\n",
    "    ('mean_imputer', mean_imputer),\n",
    "    ('en_model', en_model)\n",
    "], verbose=True)\n",
    "\n",
    "X = data_preprocessed.drop('target', axis=1)\n",
    "y = data_preprocessed['target']\n",
    "\n",
    "scores = cross_val_score(pipeline, X, y, cv=10, scoring='neg_root_mean_squared_error', error_score='raise')\n",
    "\n",
    "\n",
    "print(\"Cross-validation scores:\", -scores)\n",
    "print(\"RMSE:\", -scores.mean())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AdvancedAnalytics_1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
